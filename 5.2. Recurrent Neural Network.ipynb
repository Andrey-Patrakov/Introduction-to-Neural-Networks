{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4090882",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (Генерация текста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7928435b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences, to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dropout, Embedding\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c86e08a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/alice_in_wonderland.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e356f5",
   "metadata": {},
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5125be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка файла\n",
    "data = None\n",
    "with open(DATA_PATH, 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "corpus = data.lower().split('\\n')\n",
    "corpus = [text for text in corpus if text.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f1d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучим токенайзер на нашем тексте\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Сделаем из каждой строки по несколько последовательностей слов:\n",
    "# Мама [мыла]\n",
    "# Мама мыла [раму]\n",
    "# ...\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Сделаем все последовательности одинаковой длинны\n",
    "max_sequence_len = max([len(sequence) for sequence in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences,\n",
    "                                         maxlen=max_sequence_len,\n",
    "                                         padding='pre'))\n",
    "\n",
    "# Разобьем последовательности на входные данные и предсказания\n",
    "predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "label = to_categorical(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9cc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3d57fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 17, 128)           435072    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               394240    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              263168    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3399)              3483975   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,576,455\n",
      "Trainable params: 4,576,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 128, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(256, activity_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1024, activation='leaky_relu', kernel_regularizer=l2(0.01)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca858ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_index = {value: key for key, value in tokenizer.word_index.items()}\n",
    "\n",
    "def predict(text, count=10):\n",
    "    for _ in range(count):\n",
    "        tokens = tokenizer.texts_to_sequences([text])\n",
    "        tokens = pad_sequences(tokens, maxlen=max_sequence_len-1, padding='pre')\n",
    "        pred = model.predict(tokens, verbose=0)\n",
    "        pred = pred.argmax()\n",
    "        word = from_index[pred]\n",
    "        text += ' ' + word\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87634bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: here were doors all round the hall quiver pennyworth croqueting pennyworth leave leave wherever wherever shrieked identification\n",
      "\n",
      "Epoch 1/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 6.3696 - accuracy: 0.0537 - val_loss: 6.9887 - val_accuracy: 0.0657\n",
      "Epoch 2/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 5.8003 - accuracy: 0.0732 - val_loss: 7.0350 - val_accuracy: 0.0689\n",
      "Epoch 3/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 5.6070 - accuracy: 0.0990 - val_loss: 7.1840 - val_accuracy: 0.0868\n",
      "Epoch 4/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 5.4016 - accuracy: 0.1217 - val_loss: 7.3120 - val_accuracy: 0.1009\n",
      "Epoch 5/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 5.1981 - accuracy: 0.1416 - val_loss: 7.5015 - val_accuracy: 0.1061\n",
      "Iteration 1: here were doors all round the hall and the mock turtle was the little voice and the\n",
      "\n",
      "Epoch 1/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 5.0372 - accuracy: 0.1552 - val_loss: 7.7277 - val_accuracy: 0.1119\n",
      "Epoch 2/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 4.8908 - accuracy: 0.1709 - val_loss: 7.8403 - val_accuracy: 0.0976\n",
      "Epoch 3/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 4.7669 - accuracy: 0.1843 - val_loss: 7.9405 - val_accuracy: 0.1052\n",
      "Epoch 4/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 4.6477 - accuracy: 0.1956 - val_loss: 8.1799 - val_accuracy: 0.1077\n",
      "Epoch 5/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 4.5368 - accuracy: 0.2058 - val_loss: 8.4471 - val_accuracy: 0.1025\n",
      "Iteration 2: here were doors all round the hall and she had not a little thing to be a\n",
      "\n",
      "Epoch 1/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 4.4222 - accuracy: 0.2174 - val_loss: 8.5661 - val_accuracy: 0.0978\n",
      "Epoch 2/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 4.3292 - accuracy: 0.2298 - val_loss: 8.8233 - val_accuracy: 0.1039\n",
      "Epoch 3/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 4.2124 - accuracy: 0.2449 - val_loss: 9.1033 - val_accuracy: 0.1066\n",
      "Epoch 4/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 4.1199 - accuracy: 0.2573 - val_loss: 9.3019 - val_accuracy: 0.1022\n",
      "Epoch 5/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 4.0183 - accuracy: 0.2722 - val_loss: 9.5922 - val_accuracy: 0.0940\n",
      "Iteration 3: here were doors all round the hall and the gryphon said the gryphon in a little tone\n",
      "\n",
      "Epoch 1/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.9161 - accuracy: 0.2880 - val_loss: 9.9235 - val_accuracy: 0.1018\n",
      "Epoch 2/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.8081 - accuracy: 0.3033 - val_loss: 9.9815 - val_accuracy: 0.0944\n",
      "Epoch 3/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 3.7216 - accuracy: 0.3173 - val_loss: 10.1407 - val_accuracy: 0.0875\n",
      "Epoch 4/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.6250 - accuracy: 0.3358 - val_loss: 10.2212 - val_accuracy: 0.0923\n",
      "Epoch 5/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.5401 - accuracy: 0.3487 - val_loss: 10.4400 - val_accuracy: 0.0887\n",
      "Iteration 4: here were doors all round the hall but she was to find it out down it was\n",
      "\n",
      "Epoch 1/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.4472 - accuracy: 0.3692 - val_loss: 10.7320 - val_accuracy: 0.0939\n",
      "Epoch 2/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.3595 - accuracy: 0.3816 - val_loss: 11.0730 - val_accuracy: 0.0894\n",
      "Epoch 3/5\n",
      "706/706 [==============================] - 6s 8ms/step - loss: 3.2790 - accuracy: 0.4009 - val_loss: 11.1139 - val_accuracy: 0.0954\n",
      "Epoch 4/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 3.1943 - accuracy: 0.4193 - val_loss: 11.3629 - val_accuracy: 0.0898\n",
      "Epoch 5/5\n",
      "706/706 [==============================] - 5s 8ms/step - loss: 3.1112 - accuracy: 0.4362 - val_loss: 11.4002 - val_accuracy: 0.0862\n",
      "Iteration 5: here were doors all round the hall but she had to herself in a minute or two\n"
     ]
    }
   ],
   "source": [
    "iterations = 5\n",
    "epochs = 5\n",
    "text = 'here were doors all round the hall'\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f'Iteration {i}: {predict(text)}\\n')\n",
    "    model.fit(predictors, label,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.2,\n",
    "              verbose=1)\n",
    "\n",
    "print(f'Iteration {iterations}: {predict(text)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
